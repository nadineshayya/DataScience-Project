name: Daily Scrape and Merge

on:
  schedule:
    - cron: '0 0 * * *'  # Runs daily at midnight UTC
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Check out repository
        uses: actions/checkout@v3
        with:
          # Ensure that the provided GITHUB_TOKEN is available for subsequent Git commands
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install pandas selenium webdriver_manager beautifulsoup4

      - name: Run AllRecipes Scraper
        run: |
          # Assuming your AllRecipes scraper code is in data.py
          python data.py

      - name: Run Recipetineats Scraper
        run: |
          # Assuming your Recipetineats scraper code is in data1.py
          python data1.py

      - name: Run Spruce Eats Scraper
        run: |
          # Assuming your Spruce Eats scraper code is in scraper.py
          python scraper.py

      - name: Run Data Clean and Merge Script
        run: |
          # Assuming your cleaning and merging code is in clean_and_merge.py
          python clean_and_merge.py

      - name: Update remote URL for authentication
        run: |
          # This updates the remote URL to use the provided GITHUB_TOKEN,
          # ensuring push operations are properly authenticated.
          git remote set-url origin https://${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git

      - name: Commit and push any changes
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"
          git add .
          git commit -m "Daily data scrape and merge update [skip ci]" || echo "Nothing to commit"
          git push origin main
