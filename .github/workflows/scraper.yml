name: Daily Recipe Scraping

on:
  schedule:
    - cron: '0 3 * * *'  # Runs daily at 3am UTC
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          # Install snapd (required for snap packages) and chromium-chromedriver (for chromedriver)
          sudo apt-get install -y snapd chromium-chromedriver
          # Enable and start snapd
          sudo systemctl enable --now snapd

      - name: Install Chromium via Snap with Retries
        run: |
          echo "Installing Chromium via snap..."
          for i in {1..3}; do
            echo "Attempt $i to install Chromium via snap"
            if sudo snap install chromium; then
              echo "Chromium installed via snap successfully."
              exit 0
            else
              echo "Attempt $i failed, retrying in 5 seconds..."
              sleep 5
            fi
          done
          echo "All attempts failed. Exiting with error."
          exit 1

      - name: Install Python packages
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 requests pandas selenium lxml webdriver-manager html5lib cssselect

      - name: Scrape AllRecipes
        run: python data.py --site allrecipes
        env:
          # Use the snap-installed Chromium executable
          CHROME_PATH: /snap/bin/chromium
          # Use the installed chromedriver from apt (ensure this path is correct for your runner)
          CHROMEDRIVER_PATH: /usr/bin/chromium-chromedriver
          OUTPUT_FILE: allrecipes_limited_categories.csv

      - name: Scrape RecipeTinEats
        run: python data1.py --site recipetineats
        env:
          CHROME_PATH: /snap/bin/chromium
          CHROMEDRIVER_PATH: /usr/bin/chromium-chromedriver
          OUTPUT_FILE: recipetineats_limited_categories.csv

      - name: Scrape SpruceEats
        run: python scraper.py --site spruceeats
        env:
          CHROME_PATH: /snap/bin/chromium
          CHROMEDRIVER_PATH: /usr/bin/chromium-chromedriver
          OUTPUT_FILE: spruce_eats_recipes.csv

      - name: Clean and merge data
        run: python clean_and_merge.py

      - name: Commit and push changes
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add .
          git diff --quiet && git diff --staged --quiet || git commit -m "Auto-update scraped data"
          git push
