name: Daily Scrape and Merge

on:
  schedule:
    - cron: '0 0 * * *'  # Runs daily at midnight UTC
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    
    # Ensure this job can push code back to the repo:
    permissions:
      contents: write

    steps:
      - name: Check out repository
        uses: actions/checkout@v3
        with:
          # If you prefer to override credentials manually, set this to false
          # persist-credentials: false
          persist-credentials: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install pandas selenium webdriver_manager beautifulsoup4

      - name: Run AllRecipes Scraper
        run: |
          python data.py

      - name: Run Recipetineats Scraper
        run: |
          python data1.py

      - name: Run Spruce Eats Scraper
        run: |
          python scraper.py

      - name: Run Data Clean and Merge Script
        run: |
          python clean_and_merge.py

      - name: Update remote URL for authentication
        run: |
          # Use the x-access-token form so that Git can authenticate using GITHUB_TOKEN
          git remote set-url origin "https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git"

      - name: Commit and push any changes
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"

          # Stage files
          git add .

          # Commit only if there are changes
          if git diff --staged --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Daily data scrape and merge update [skip ci]"
            git push origin main
          fi
